{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Capstone Project - Advanced Machine Learning\n",
    "## TEC-VIII Programa de Especializaci√≥n en Big Data Analytics aplicada a los Negocios\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Informaci√≥n del Proyecto\n",
    "\n",
    "| Campo | Informaci√≥n |\n",
    "|-------|-------------|\n",
    "| **Nombre del Estudiante** | [Completar] |\n",
    "| **T√≠tulo del Proyecto** | [Completar] |\n",
    "| **Fecha de Entrega** | [Completar] |\n",
    "| **Profesor** | [Completar] |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìë √çndice\n",
    "\n",
    "1. [Resumen Ejecutivo](#1-resumen-ejecutivo)\n",
    "2. [Configuraci√≥n del Entorno](#2-configuraci√≥n-del-entorno)\n",
    "3. [Definici√≥n del Problema de Negocio](#3-definici√≥n-del-problema-de-negocio)\n",
    "4. [Carga y Exploraci√≥n de Datos](#4-carga-y-exploraci√≥n-de-datos)\n",
    "5. [Preprocesamiento de Datos](#5-preprocesamiento-de-datos)\n",
    "6. [Dise√±o y Arquitectura del Modelo](#6-dise√±o-y-arquitectura-del-modelo)\n",
    "7. [Entrenamiento del Modelo](#7-entrenamiento-del-modelo)\n",
    "8. [Evaluaci√≥n y M√©tricas](#8-evaluaci√≥n-y-m√©tricas)\n",
    "9. [Interpretaci√≥n de Resultados](#9-interpretaci√≥n-de-resultados)\n",
    "10. [Conclusiones y Recomendaciones de Negocio](#10-conclusiones-y-recomendaciones-de-negocio)\n",
    "11. [Referencias](#11-referencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Resumen Ejecutivo\n",
    "\n",
    "**Instrucciones:** Proporcione un resumen conciso (m√°ximo 300 palabras) que incluya:\n",
    "- Problema de negocio abordado\n",
    "- Metodolog√≠a utilizada\n",
    "- Principales hallazgos\n",
    "- Impacto esperado en el negocio\n",
    "\n",
    "---\n",
    "\n",
    "*[Escriba su resumen ejecutivo aqu√≠]*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n del Entorno\n",
    "\n",
    "### 2.1 Verificaci√≥n de GPU (Recomendado para Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay GPU disponible\n",
    "import torch\n",
    "\n",
    "# Verificar disponibilidad de GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU no disponible. Usando CPU.\")\n",
    "    print(\"   Recomendaci√≥n: En Colab, vaya a Runtime > Change runtime type > GPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"\\nDispositivo seleccionado: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Instalaci√≥n de Librer√≠as Adicionales (si es necesario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente e instale las librer√≠as adicionales que necesite\n",
    "# !pip install transformers\n",
    "# !pip install pytorch-lightning\n",
    "# !pip install optuna\n",
    "# !pip install shap\n",
    "# !pip install lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# LIBRER√çAS FUNDAMENTALES\n",
    "# =====================================================\n",
    "\n",
    "# Manipulaci√≥n de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "# Deep Learning - TensorFlow/Keras (alternativa)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Preprocesamiento\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report,\n",
    "                             mean_squared_error, mean_absolute_error, r2_score)\n",
    "\n",
    "# Utilidades\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"‚úÖ Todas las librer√≠as importadas correctamente\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Conexi√≥n con Google Drive (para cargar datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montar Google Drive para acceder a los datos\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Definir la ruta base de su proyecto\n",
    "# Modifique esta ruta seg√∫n la ubicaci√≥n de sus datos\n",
    "BASE_PATH = '/content/drive/MyDrive/Capstone_Project/'\n",
    "\n",
    "print(f\"‚úÖ Google Drive montado\")\n",
    "print(f\"   Ruta base del proyecto: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Definici√≥n del Problema de Negocio\n",
    "\n",
    "### 3.1 Contexto del Negocio\n",
    "\n",
    "**Instrucciones:** Describa el contexto empresarial, incluyendo:\n",
    "- Industria/Sector\n",
    "- Empresa o caso de estudio\n",
    "- Situaci√≥n actual\n",
    "\n",
    "---\n",
    "\n",
    "*[Describa el contexto del negocio aqu√≠]*\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Problema a Resolver\n",
    "\n",
    "**Instrucciones:** Defina claramente:\n",
    "- ¬øCu√°l es el problema espec√≠fico?\n",
    "- ¬øPor qu√© es importante resolverlo?\n",
    "- ¬øCu√°l es el impacto actual del problema?\n",
    "\n",
    "---\n",
    "\n",
    "*[Describa el problema aqu√≠]*\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Objetivos del Proyecto\n",
    "\n",
    "**Instrucciones:** Liste los objetivos SMART (Espec√≠ficos, Medibles, Alcanzables, Relevantes, Temporales)\n",
    "\n",
    "---\n",
    "\n",
    "**Objetivo General:**\n",
    "*[Completar]*\n",
    "\n",
    "**Objetivos Espec√≠ficos:**\n",
    "1. *[Completar]*\n",
    "2. *[Completar]*\n",
    "3. *[Completar]*\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Tipo de Problema de Machine Learning\n",
    "\n",
    "**Instrucciones:** Identifique el tipo de problema:\n",
    "- [ ] Clasificaci√≥n binaria\n",
    "- [ ] Clasificaci√≥n multiclase\n",
    "- [ ] Regresi√≥n\n",
    "- [ ] Clustering\n",
    "- [ ] Series temporales\n",
    "- [ ] Procesamiento de Lenguaje Natural (NLP)\n",
    "- [ ] Visi√≥n por Computadora\n",
    "- [ ] Otro: _________\n",
    "\n",
    "**Justificaci√≥n:**\n",
    "*[Explique por qu√© este tipo de problema es apropiado]*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Carga y Exploraci√≥n de Datos\n",
    "\n",
    "### 4.1 Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# CARGA DE DATOS\n",
    "# =====================================================\n",
    "\n",
    "# Opci√≥n 1: Cargar desde Google Drive\n",
    "# df = pd.read_csv(BASE_PATH + 'datos.csv')\n",
    "\n",
    "# Opci√≥n 2: Cargar desde URL\n",
    "# df = pd.read_csv('https://url-de-sus-datos.com/datos.csv')\n",
    "\n",
    "# Opci√≥n 3: Cargar desde archivo local (subido a Colab)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# df = pd.read_csv('nombre_archivo.csv')\n",
    "\n",
    "# Opci√≥n 4: Dataset de ejemplo (para testing)\n",
    "# from sklearn.datasets import load_iris, load_boston, fetch_california_housing\n",
    "# data = load_iris()\n",
    "# df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "# df['target'] = data.target\n",
    "\n",
    "# =====================================================\n",
    "# COMPLETE AQU√ç: Cargue su dataset\n",
    "# =====================================================\n",
    "\n",
    "# df = pd.read_csv('...')  # Descomente y complete\n",
    "\n",
    "print(f\"‚úÖ Dataset cargado exitosamente\")\n",
    "print(f\"   Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Descripci√≥n del Dataset\n",
    "\n",
    "**Instrucciones:** Describa su dataset:\n",
    "- Fuente de los datos\n",
    "- Per√≠odo de tiempo que cubren\n",
    "- Descripci√≥n de cada variable\n",
    "\n",
    "---\n",
    "\n",
    "| Variable | Tipo | Descripci√≥n |\n",
    "|----------|------|-------------|\n",
    "| variable_1 | num√©rica/categ√≥rica | [Descripci√≥n] |\n",
    "| variable_2 | num√©rica/categ√≥rica | [Descripci√≥n] |\n",
    "| ... | ... | ... |\n",
    "| target | num√©rica/categ√≥rica | [Variable objetivo] |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Exploraci√≥n Inicial de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# INFORMACI√ìN GENERAL DEL DATASET\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFORMACI√ìN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Primeras filas\n",
    "print(\"\\nüìä Primeras 5 filas:\")\n",
    "display(df.head())\n",
    "\n",
    "# Informaci√≥n del dataset\n",
    "print(\"\\nüìã Informaci√≥n del Dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\nüìà Estad√≠sticas Descriptivas:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# AN√ÅLISIS DE VALORES FALTANTES\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AN√ÅLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calcular valores faltantes\n",
    "missing_data = pd.DataFrame({\n",
    "    'Total Faltantes': df.isnull().sum(),\n",
    "    'Porcentaje (%)': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "missing_data = missing_data[missing_data['Total Faltantes'] > 0].sort_values('Porcentaje (%)', ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    print(\"\\n‚ö†Ô∏è Variables con valores faltantes:\")\n",
    "    display(missing_data)\n",
    "    \n",
    "    # Visualizaci√≥n de valores faltantes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=missing_data.index, y='Porcentaje (%)', data=missing_data)\n",
    "    plt.title('Porcentaje de Valores Faltantes por Variable')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Porcentaje (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n‚úÖ No hay valores faltantes en el dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# AN√ÅLISIS DE LA VARIABLE OBJETIVO\n",
    "# =====================================================\n",
    "\n",
    "# COMPLETE: Especifique el nombre de su variable objetivo\n",
    "TARGET_COLUMN = 'target'  # Cambie 'target' por el nombre de su variable objetivo\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"AN√ÅLISIS DE LA VARIABLE OBJETIVO: {TARGET_COLUMN}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Para clasificaci√≥n\n",
    "if df[TARGET_COLUMN].dtype == 'object' or df[TARGET_COLUMN].nunique() < 20:\n",
    "    print(\"\\nüìä Distribuci√≥n de clases:\")\n",
    "    class_dist = df[TARGET_COLUMN].value_counts()\n",
    "    print(class_dist)\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Gr√°fico de barras\n",
    "    sns.countplot(data=df, x=TARGET_COLUMN, ax=axes[0])\n",
    "    axes[0].set_title(f'Distribuci√≥n de {TARGET_COLUMN}')\n",
    "    axes[0].set_xlabel(TARGET_COLUMN)\n",
    "    axes[0].set_ylabel('Frecuencia')\n",
    "    \n",
    "    # Gr√°fico de pastel\n",
    "    axes[1].pie(class_dist.values, labels=class_dist.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[1].set_title(f'Proporci√≥n de {TARGET_COLUMN}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Verificar desbalance\n",
    "    imbalance_ratio = class_dist.max() / class_dist.min()\n",
    "    if imbalance_ratio > 3:\n",
    "        print(f\"\\n‚ö†Ô∏è ADVERTENCIA: Dataset desbalanceado (ratio {imbalance_ratio:.2f}:1)\")\n",
    "        print(\"   Considere t√©cnicas de balanceo: SMOTE, undersampling, class weights\")\n",
    "else:\n",
    "    # Para regresi√≥n\n",
    "    print(\"\\nüìä Estad√≠sticas de la variable objetivo:\")\n",
    "    print(df[TARGET_COLUMN].describe())\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histograma\n",
    "    sns.histplot(df[TARGET_COLUMN], kde=True, ax=axes[0])\n",
    "    axes[0].set_title(f'Distribuci√≥n de {TARGET_COLUMN}')\n",
    "    \n",
    "    # Box plot\n",
    "    sns.boxplot(y=df[TARGET_COLUMN], ax=axes[1])\n",
    "    axes[1].set_title(f'Box Plot de {TARGET_COLUMN}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# AN√ÅLISIS DE CORRELACIONES\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MATRIZ DE CORRELACIONES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Seleccionar solo columnas num√©ricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_cols) > 1:\n",
    "    # Calcular correlaciones\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, fmt='.2f', linewidths=0.5)\n",
    "    plt.title('Matriz de Correlaciones')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlaciones con la variable objetivo\n",
    "    if TARGET_COLUMN in numeric_cols:\n",
    "        print(f\"\\nüìä Correlaciones con {TARGET_COLUMN}:\")\n",
    "        target_corr = correlation_matrix[TARGET_COLUMN].drop(TARGET_COLUMN).sort_values(ascending=False)\n",
    "        print(target_corr)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay suficientes columnas num√©ricas para an√°lisis de correlaci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# VISUALIZACIONES ADICIONALES\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUALIZACIONES ADICIONALES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Distribuci√≥n de variables num√©ricas\n",
    "numeric_cols_plot = df.select_dtypes(include=[np.number]).columns[:8]  # Primeras 8 columnas\n",
    "\n",
    "if len(numeric_cols_plot) > 0:\n",
    "    n_cols = 2\n",
    "    n_rows = (len(numeric_cols_plot) + 1) // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols_plot):\n",
    "        if i < len(axes):\n",
    "            sns.histplot(df[col], kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f'Distribuci√≥n de {col}')\n",
    "    \n",
    "    # Ocultar ejes vac√≠os\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Hallazgos del EDA\n",
    "\n",
    "**Instrucciones:** Resuma los principales hallazgos de la exploraci√≥n de datos:\n",
    "\n",
    "---\n",
    "\n",
    "**Hallazgos Principales:**\n",
    "1. *[Completar]*\n",
    "2. *[Completar]*\n",
    "3. *[Completar]*\n",
    "\n",
    "**Problemas Identificados:**\n",
    "1. *[Completar]*\n",
    "2. *[Completar]*\n",
    "\n",
    "**Acciones a Tomar:**\n",
    "1. *[Completar]*\n",
    "2. *[Completar]*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Preprocesamiento de Datos\n",
    "\n",
    "### 5.1 Tratamiento de Valores Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# TRATAMIENTO DE VALORES FALTANTES\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRATAMIENTO DE VALORES FALTANTES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Crear copia del dataframe\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Opci√≥n 1: Eliminar filas con valores faltantes\n",
    "# df_clean = df_clean.dropna()\n",
    "\n",
    "# Opci√≥n 2: Imputar con la media (variables num√©ricas)\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# imputer = SimpleImputer(strategy='mean')\n",
    "# df_clean[numeric_cols] = imputer.fit_transform(df_clean[numeric_cols])\n",
    "\n",
    "# Opci√≥n 3: Imputar con la moda (variables categ√≥ricas)\n",
    "# for col in categorical_cols:\n",
    "#     df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "\n",
    "# Opci√≥n 4: Imputaci√≥n avanzada con KNN\n",
    "# from sklearn.impute import KNNImputer\n",
    "# imputer = KNNImputer(n_neighbors=5)\n",
    "# df_clean[numeric_cols] = imputer.fit_transform(df_clean[numeric_cols])\n",
    "\n",
    "# =====================================================\n",
    "# COMPLETE AQU√ç: Aplique su estrategia de imputaci√≥n\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n‚úÖ Valores faltantes tratados\")\n",
    "print(f\"   Filas restantes: {len(df_clean):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Tratamiento de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# DETECCI√ìN Y TRATAMIENTO DE OUTLIERS\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DETECCI√ìN DE OUTLIERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detecta outliers usando el m√©todo IQR\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "# Detectar outliers en cada columna num√©rica\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "outlier_summary = []\n",
    "for col in numeric_cols:\n",
    "    n_outliers, lower, upper = detect_outliers_iqr(df_clean, col)\n",
    "    if n_outliers > 0:\n",
    "        outlier_summary.append({\n",
    "            'Variable': col,\n",
    "            'N_Outliers': n_outliers,\n",
    "            'Porcentaje (%)': round(n_outliers/len(df_clean)*100, 2),\n",
    "            'L√≠mite_Inferior': round(lower, 2),\n",
    "            'L√≠mite_Superior': round(upper, 2)\n",
    "        })\n",
    "\n",
    "if outlier_summary:\n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    print(\"\\n‚ö†Ô∏è Variables con outliers detectados:\")\n",
    "    display(outlier_df)\n",
    "else:\n",
    "    print(\"\\n‚úÖ No se detectaron outliers significativos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# TRATAMIENTO DE OUTLIERS (OPCIONAL)\n",
    "# =====================================================\n",
    "\n",
    "# Opci√≥n 1: Eliminar outliers\n",
    "# for col in numeric_cols:\n",
    "#     Q1, Q3 = df_clean[col].quantile([0.25, 0.75])\n",
    "#     IQR = Q3 - Q1\n",
    "#     df_clean = df_clean[(df_clean[col] >= Q1 - 1.5*IQR) & (df_clean[col] <= Q3 + 1.5*IQR)]\n",
    "\n",
    "# Opci√≥n 2: Capear outliers (winsorizing)\n",
    "# from scipy.stats import mstats\n",
    "# for col in numeric_cols:\n",
    "#     df_clean[col] = mstats.winsorize(df_clean[col], limits=[0.05, 0.05])\n",
    "\n",
    "# Opci√≥n 3: Transformaci√≥n logar√≠tmica\n",
    "# for col in cols_to_transform:\n",
    "#     df_clean[col] = np.log1p(df_clean[col])\n",
    "\n",
    "# =====================================================\n",
    "# COMPLETE AQU√ç: Aplique su estrategia de tratamiento\n",
    "# =====================================================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Codificaci√≥n de Variables Categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identificar variables categ√≥ricas\n",
    "categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"\\nVariables categ√≥ricas encontradas: {categorical_cols}\")\n",
    "\n",
    "# Opci√≥n 1: Label Encoding (para variables ordinales o target)\n",
    "# le = LabelEncoder()\n",
    "# df_clean['columna_encoded'] = le.fit_transform(df_clean['columna'])\n",
    "\n",
    "# Opci√≥n 2: One-Hot Encoding (para variables nominales)\n",
    "# df_clean = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Opci√≥n 3: Target Encoding\n",
    "# from sklearn.preprocessing import TargetEncoder\n",
    "# encoder = TargetEncoder()\n",
    "# df_clean[categorical_cols] = encoder.fit_transform(df_clean[categorical_cols], df_clean[TARGET_COLUMN])\n",
    "\n",
    "# =====================================================\n",
    "# COMPLETE AQU√ç: Aplique su estrategia de codificaci√≥n\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n‚úÖ Codificaci√≥n completada\")\n",
    "print(f\"   Dimensiones finales: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Escalado/Normalizaci√≥n de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# ESCALADO DE FEATURES\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ESCALADO DE FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Separar features y target\n",
    "X = df_clean.drop(columns=[TARGET_COLUMN])\n",
    "y = df_clean[TARGET_COLUMN]\n",
    "\n",
    "print(f\"\\nDimensiones de X: {X.shape}\")\n",
    "print(f\"Dimensiones de y: {y.shape}\")\n",
    "\n",
    "# Opci√≥n 1: StandardScaler (media=0, std=1) - Recomendado para redes neuronales\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Opci√≥n 2: MinMaxScaler (rango [0,1])\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# Opci√≥n 3: RobustScaler (robusto a outliers)\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# scaler = RobustScaler()\n",
    "\n",
    "# Aplicar escalado\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "print(f\"\\n‚úÖ Escalado completado usando {type(scaler).__name__}\")\n",
    "print(f\"   Media de features: {X_scaled.mean().mean():.6f}\")\n",
    "print(f\"   Std de features: {X_scaled.std().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Divisi√≥n de Datos (Train/Validation/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# DIVISI√ìN DE DATOS\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DIVISI√ìN DE DATOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Divisi√≥n en train (70%), validation (15%), test (15%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.15, random_state=RANDOM_SEED, stratify=y if y.dtype == 'object' or y.nunique() < 20 else None\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=RANDOM_SEED, stratify=y_temp if y_temp.dtype == 'object' or y_temp.nunique() < 20 else None  # 0.176 ‚âà 15% del total\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Divisi√≥n de datos:\")\n",
    "print(f\"   Training set:   {X_train.shape[0]:,} muestras ({X_train.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"   Validation set: {X_val.shape[0]:,} muestras ({X_val.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"   Test set:       {X_test.shape[0]:,} muestras ({X_test.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "\n",
    "# Verificar distribuci√≥n de clases (para clasificaci√≥n)\n",
    "if y.dtype == 'object' or y.nunique() < 20:\n",
    "    print(f\"\\nüìä Distribuci√≥n de clases en cada conjunto:\")\n",
    "    print(f\"   Train: {dict(y_train.value_counts(normalize=True).round(3))}\")\n",
    "    print(f\"   Val:   {dict(y_val.value_counts(normalize=True).round(3))}\")\n",
    "    print(f\"   Test:  {dict(y_test.value_counts(normalize=True).round(3))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Preparaci√≥n de Datos para Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# PREPARACI√ìN PARA PYTORCH\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPARACI√ìN DE DATOS PARA PYTORCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convertir a tensores de PyTorch\n",
    "X_train_tensor = torch.FloatTensor(X_train.values)\n",
    "X_val_tensor = torch.FloatTensor(X_val.values)\n",
    "X_test_tensor = torch.FloatTensor(X_test.values)\n",
    "\n",
    "# Para clasificaci√≥n\n",
    "if y.dtype == 'object' or y.nunique() < 20:\n",
    "    # Codificar labels si es necesario\n",
    "    if y_train.dtype == 'object':\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "        y_val_encoded = label_encoder.transform(y_val)\n",
    "        y_test_encoded = label_encoder.transform(y_test)\n",
    "    else:\n",
    "        y_train_encoded = y_train.values\n",
    "        y_val_encoded = y_val.values\n",
    "        y_test_encoded = y_test.values\n",
    "    \n",
    "    y_train_tensor = torch.LongTensor(y_train_encoded)\n",
    "    y_val_tensor = torch.LongTensor(y_val_encoded)\n",
    "    y_test_tensor = torch.LongTensor(y_test_encoded)\n",
    "else:\n",
    "    # Para regresi√≥n\n",
    "    y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)\n",
    "    y_val_tensor = torch.FloatTensor(y_val.values).unsqueeze(1)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1)\n",
    "\n",
    "# Crear DataLoaders\n",
    "BATCH_SIZE = 32  # Ajuste seg√∫n su dataset\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoaders creados\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Batches de entrenamiento: {len(train_loader)}\")\n",
    "print(f\"   Batches de validaci√≥n: {len(val_loader)}\")\n",
    "print(f\"   Batches de test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# PREPARACI√ìN PARA TENSORFLOW/KERAS (ALTERNATIVA)\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPARACI√ìN DE DATOS PARA TENSORFLOW/KERAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convertir a arrays numpy (Keras acepta DataFrames directamente, pero es mejor convertir)\n",
    "X_train_np = X_train.values.astype('float32')\n",
    "X_val_np = X_val.values.astype('float32')\n",
    "X_test_np = X_test.values.astype('float32')\n",
    "\n",
    "# Para clasificaci√≥n: One-hot encoding del target\n",
    "if y.dtype == 'object' or y.nunique() < 20:\n",
    "    num_classes = y.nunique()\n",
    "    y_train_np = keras.utils.to_categorical(y_train_encoded, num_classes)\n",
    "    y_val_np = keras.utils.to_categorical(y_val_encoded, num_classes)\n",
    "    y_test_np = keras.utils.to_categorical(y_test_encoded, num_classes)\n",
    "else:\n",
    "    y_train_np = y_train.values.astype('float32')\n",
    "    y_val_np = y_val.values.astype('float32')\n",
    "    y_test_np = y_test.values.astype('float32')\n",
    "\n",
    "print(f\"\\n‚úÖ Datos preparados para TensorFlow/Keras\")\n",
    "print(f\"   Shape X_train: {X_train_np.shape}\")\n",
    "print(f\"   Shape y_train: {y_train_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Dise√±o y Arquitectura del Modelo\n",
    "\n",
    "### 6.1 Justificaci√≥n de la Arquitectura\n",
    "\n",
    "**Instrucciones:** Justifique la elecci√≥n de su arquitectura de red neuronal:\n",
    "- ¬øPor qu√© eligi√≥ este tipo de arquitectura?\n",
    "- ¬øQu√© alternativas consider√≥?\n",
    "- ¬øC√≥mo determin√≥ el n√∫mero de capas y neuronas?\n",
    "\n",
    "---\n",
    "\n",
    "*[Escriba su justificaci√≥n aqu√≠]*\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 Definici√≥n del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# DEFINICI√ìN DEL MODELO CON PYTORCH\n",
    "# =====================================================\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Red Neuronal para [Clasificaci√≥n/Regresi√≥n]\n",
    "    \n",
    "    Arquitectura:\n",
    "    - Capa de entrada: [n_features] neuronas\n",
    "    - Capas ocultas: [Describir]\n",
    "    - Capa de salida: [n_outputs] neuronas\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.3):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Capas ocultas\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Capa de salida\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURACI√ìN DEL MODELO\n",
    "# =====================================================\n",
    "\n",
    "INPUT_SIZE = X_train.shape[1]\n",
    "HIDDEN_SIZES = [128, 64, 32]  # Ajuste seg√∫n su problema\n",
    "OUTPUT_SIZE = y.nunique() if (y.dtype == 'object' or y.nunique() < 20) else 1\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "# Crear modelo\n",
    "model_pytorch = NeuralNetwork(INPUT_SIZE, HIDDEN_SIZES, OUTPUT_SIZE, DROPOUT_RATE)\n",
    "model_pytorch = model_pytorch.to(device)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ARQUITECTURA DEL MODELO (PyTorch)\")\n",
    "print(\"=\" * 60)\n",
    "print(model_pytorch)\n",
    "\n",
    "# Contar par√°metros\n",
    "total_params = sum(p.numel() for p in model_pytorch.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_pytorch.parameters() if p.requires_grad)\n",
    "print(f\"\\nüìä Par√°metros totales: {total_params:,}\")\n",
    "print(f\"   Par√°metros entrenables: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# DEFINICI√ìN DEL MODELO CON KERAS (ALTERNATIVA)\n",
    "# =====================================================\n",
    "\n",
    "def create_keras_model(input_shape, hidden_sizes, output_size, dropout_rate=0.3, task='classification'):\n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal con Keras.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Dimensi√≥n de entrada\n",
    "        hidden_sizes: Lista con el n√∫mero de neuronas por capa oculta\n",
    "        output_size: N√∫mero de neuronas de salida\n",
    "        dropout_rate: Tasa de dropout\n",
    "        task: 'classification' o 'regression'\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Capa de entrada\n",
    "    model.add(layers.Input(shape=(input_shape,)))\n",
    "    \n",
    "    # Capas ocultas\n",
    "    for hidden_size in hidden_sizes:\n",
    "        model.add(layers.Dense(hidden_size))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Capa de salida\n",
    "    if task == 'classification':\n",
    "        if output_size == 2:\n",
    "            model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        else:\n",
    "            model.add(layers.Dense(output_size, activation='softmax'))\n",
    "    else:\n",
    "        model.add(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo Keras\n",
    "TASK = 'classification'  # Cambie a 'regression' si es necesario\n",
    "\n",
    "model_keras = create_keras_model(\n",
    "    input_shape=INPUT_SIZE,\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    output_size=OUTPUT_SIZE,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    task=TASK\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ARQUITECTURA DEL MODELO (Keras)\")\n",
    "print(\"=\" * 60)\n",
    "model_keras.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Diagrama de la Arquitectura\n",
    "\n",
    "**Instrucciones:** Incluya un diagrama visual de su arquitectura de red neuronal.\n",
    "\n",
    "---\n",
    "\n",
    "*[Inserte diagrama o descripci√≥n visual de la arquitectura]*\n",
    "\n",
    "```\n",
    "Input Layer          Hidden Layer 1       Hidden Layer 2       Output Layer\n",
    "[n features]   -->   [128 neurons]   -->  [64 neurons]    -->  [n classes]\n",
    "                     + BatchNorm          + BatchNorm\n",
    "                     + ReLU               + ReLU\n",
    "                     + Dropout(0.3)       + Dropout(0.3)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Entrenamiento del Modelo\n",
    "\n",
    "### 7.1 Configuraci√≥n del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# HIPERPAR√ÅMETROS DE ENTRENAMIENTO\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURACI√ìN DEL ENTRENAMIENTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Hiperpar√°metros\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "\n",
    "print(f\"\\nüìã Hiperpar√°metros:\")\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# CONFIGURACI√ìN DE LOSS Y OPTIMIZADOR (PyTorch)\n",
    "# =====================================================\n",
    "\n",
    "# Seleccionar funci√≥n de p√©rdida seg√∫n el tipo de problema\n",
    "if y.dtype == 'object' or y.nunique() < 20:\n",
    "    # Clasificaci√≥n\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    task_type = 'classification'\n",
    "else:\n",
    "    # Regresi√≥n\n",
    "    criterion = nn.MSELoss()\n",
    "    task_type = 'regression'\n",
    "\n",
    "# Optimizador\n",
    "optimizer = optim.Adam(model_pytorch.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüìã Configuraci√≥n:\")\n",
    "print(f\"   Tipo de problema: {task_type}\")\n",
    "print(f\"   Funci√≥n de p√©rdida: {criterion}\")\n",
    "print(f\"   Optimizador: Adam\")\n",
    "print(f\"   Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Entrenamiento del Modelo (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# FUNCIONES DE ENTRENAMIENTO Y EVALUACI√ìN\n",
    "# =====================================================\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Entrena el modelo por una √©poca.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if task_type == 'classification':\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total if task_type == 'classification' else None\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    \"\"\"Eval√∫a el modelo en el conjunto de validaci√≥n.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if task_type == 'classification':\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct / total if task_type == 'classification' else None\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# ENTRENAMIENTO DEL MODELO (PyTorch)\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENTRENAMIENTO DEL MODELO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Historial de entrenamiento\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(f\"\\nüöÄ Iniciando entrenamiento...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Entrenamiento\n",
    "    train_loss, train_acc = train_epoch(model_pytorch, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validaci√≥n\n",
    "    val_loss, val_acc = evaluate(model_pytorch, val_loader, criterion, device)\n",
    "    \n",
    "    # Guardar historial\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    if task_type == 'classification':\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Imprimir progreso cada 10 √©pocas\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        if task_type == 'classification':\n",
    "            print(f\"√âpoca {epoch+1:3d}/{EPOCHS} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        else:\n",
    "            print(f\"√âpoca {epoch+1:3d}/{EPOCHS} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model_pytorch.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\n‚ö†Ô∏è Early stopping en √©poca {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Cargar mejor modelo\n",
    "if best_model_state is not None:\n",
    "    model_pytorch.load_state_dict(best_model_state)\n",
    "    print(f\"\\n‚úÖ Mejor modelo cargado (Val Loss: {best_val_loss:.4f})\")\n",
    "\n",
    "print(f\"\\nüéâ Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Entrenamiento del Modelo (Keras - Alternativa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# ENTRENAMIENTO DEL MODELO (KERAS)\n",
    "# =====================================================\n",
    "\n",
    "# Compilar modelo\n",
    "if TASK == 'classification':\n",
    "    if OUTPUT_SIZE == 2:\n",
    "        model_keras.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    else:\n",
    "        model_keras.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "else:\n",
    "    model_keras.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "# Callbacks\n",
    "keras_callbacks = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=EARLY_STOPPING_PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "# Entrenar\n",
    "print(\"=\" * 60)\n",
    "print(\"ENTRENAMIENTO DEL MODELO (KERAS)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history_keras = model_keras.fit(\n",
    "    X_train_np, y_train_np,\n",
    "    validation_data=(X_val_np, y_val_np),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=keras_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Visualizaci√≥n del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# VISUALIZACI√ìN DEL PROCESO DE ENTRENAMIENTO\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CURVAS DE APRENDIZAJE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gr√°fico de p√©rdida\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('Evoluci√≥n de la P√©rdida', fontsize=14)\n",
    "axes[0].set_xlabel('√âpoca')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico de precisi√≥n (solo para clasificaci√≥n)\n",
    "if task_type == 'classification':\n",
    "    axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "    axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[1].set_title('Evoluci√≥n de la Precisi√≥n', fontsize=14)\n",
    "    axes[1].set_xlabel('√âpoca')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'N/A para Regresi√≥n', ha='center', va='center', fontsize=14)\n",
    "    axes[1].set_title('Precisi√≥n (No aplica)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis del entrenamiento\n",
    "print(\"\\nüìä An√°lisis del Entrenamiento:\")\n",
    "print(f\"   √âpocas completadas: {len(history['train_loss'])}\")\n",
    "print(f\"   Mejor val_loss: {min(history['val_loss']):.4f} (√©poca {history['val_loss'].index(min(history['val_loss']))+1})\")\n",
    "if task_type == 'classification':\n",
    "    print(f\"   Mejor val_acc: {max(history['val_acc']):.4f} (√©poca {history['val_acc'].index(max(history['val_acc']))+1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Evaluaci√≥n y M√©tricas\n",
    "\n",
    "### 8.1 Evaluaci√≥n en el Conjunto de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# EVALUACI√ìN EN EL CONJUNTO DE TEST\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUACI√ìN EN CONJUNTO DE TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Hacer predicciones\n",
    "model_pytorch.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_device = X_test_tensor.to(device)\n",
    "    outputs = model_pytorch(X_test_device)\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "        y_true = y_test_tensor.numpy()\n",
    "        y_proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "    else:\n",
    "        y_pred = outputs.cpu().numpy().flatten()\n",
    "        y_true = y_test_tensor.numpy().flatten()\n",
    "\n",
    "print(f\"\\n‚úÖ Predicciones realizadas: {len(y_pred)} muestras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# M√âTRICAS DE CLASIFICACI√ìN\n",
    "# =====================================================\n",
    "\n",
    "if task_type == 'classification':\n",
    "    print(\"=\" * 60)\n",
    "    print(\"M√âTRICAS DE CLASIFICACI√ìN\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\nüìä M√©tricas Principales:\")\n",
    "    print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall:    {recall:.4f}\")\n",
    "    print(f\"   F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Reporte de clasificaci√≥n completo\n",
    "    print(f\"\\nüìã Reporte de Clasificaci√≥n Detallado:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    # Matriz de confusi√≥n\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=range(OUTPUT_SIZE), \n",
    "                yticklabels=range(OUTPUT_SIZE))\n",
    "    plt.title('Matriz de Confusi√≥n', fontsize=14)\n",
    "    plt.xlabel('Predicci√≥n')\n",
    "    plt.ylabel('Valor Real')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# M√âTRICAS DE REGRESI√ìN\n",
    "# =====================================================\n",
    "\n",
    "if task_type == 'regression':\n",
    "    print(\"=\" * 60)\n",
    "    print(\"M√âTRICAS DE REGRESI√ìN\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nüìä M√©tricas de Regresi√≥n:\")\n",
    "    print(f\"   MSE:  {mse:.4f}\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    print(f\"   MAE:  {mae:.4f}\")\n",
    "    print(f\"   R¬≤:   {r2:.4f}\")\n",
    "    \n",
    "    # Gr√°fico de predicciones vs valores reales\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[0].scatter(y_true, y_pred, alpha=0.5)\n",
    "    axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    axes[0].set_xlabel('Valor Real')\n",
    "    axes[0].set_ylabel('Predicci√≥n')\n",
    "    axes[0].set_title('Predicciones vs Valores Reales')\n",
    "    \n",
    "    # Distribuci√≥n de residuos\n",
    "    residuos = y_true - y_pred\n",
    "    axes[1].hist(residuos, bins=50, edgecolor='black')\n",
    "    axes[1].axvline(x=0, color='r', linestyle='--')\n",
    "    axes[1].set_xlabel('Residuo')\n",
    "    axes[1].set_ylabel('Frecuencia')\n",
    "    axes[1].set_title('Distribuci√≥n de Residuos')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Comparaci√≥n con Modelo Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# COMPARACI√ìN CON MODELO BASELINE\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARACI√ìN CON MODELO BASELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if task_type == 'classification':\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    # Modelos baseline\n",
    "    baselines = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_SEED),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
    "    }\n",
    "else:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    baselines = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
    "    }\n",
    "\n",
    "# Entrenar y evaluar baselines\n",
    "results = {'Modelo': [], 'M√©trica': []}\n",
    "\n",
    "for name, model in baselines.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_baseline = model.predict(X_test)\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        metric = accuracy_score(y_test, y_pred_baseline)\n",
    "        metric_name = 'Accuracy'\n",
    "    else:\n",
    "        metric = r2_score(y_test, y_pred_baseline)\n",
    "        metric_name = 'R¬≤'\n",
    "    \n",
    "    results['Modelo'].append(name)\n",
    "    results['M√©trica'].append(metric)\n",
    "\n",
    "# Agregar modelo de Deep Learning\n",
    "results['Modelo'].append('Deep Learning')\n",
    "if task_type == 'classification':\n",
    "    results['M√©trica'].append(accuracy)\n",
    "else:\n",
    "    results['M√©trica'].append(r2)\n",
    "\n",
    "# Mostrar comparaci√≥n\n",
    "comparison_df = pd.DataFrame(results)\n",
    "comparison_df = comparison_df.sort_values('M√©trica', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Comparaci√≥n de Modelos ({metric_name}):\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#2ecc71' if m == 'Deep Learning' else '#3498db' for m in comparison_df['Modelo']]\n",
    "plt.barh(comparison_df['Modelo'], comparison_df['M√©trica'], color=colors)\n",
    "plt.xlabel(metric_name)\n",
    "plt.title(f'Comparaci√≥n de Modelos - {metric_name}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 An√°lisis de Resultados\n",
    "\n",
    "**Instrucciones:** Analice los resultados obtenidos:\n",
    "\n",
    "---\n",
    "\n",
    "**Rendimiento del Modelo:**\n",
    "*[Analice las m√©tricas obtenidas]*\n",
    "\n",
    "**Comparaci√≥n con Baselines:**\n",
    "*[Compare el rendimiento con los modelos baseline]*\n",
    "\n",
    "**Fortalezas del Modelo:**\n",
    "1. *[Completar]*\n",
    "2. *[Completar]*\n",
    "\n",
    "**Debilidades del Modelo:**\n",
    "1. *[Completar]*\n",
    "2. *[Completar]*\n",
    "\n",
    "**Posibles Mejoras:**\n",
    "1. *[Completar]*\n",
    "2. *[Completar]*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Interpretaci√≥n de Resultados\n",
    "\n",
    "### 9.1 Importancia de Features (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# INTERPRETABILIDAD CON SHAP (OPCIONAL)\n",
    "# =====================================================\n",
    "\n",
    "# Instalar SHAP si no est√° disponible\n",
    "# !pip install shap\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"AN√ÅLISIS DE IMPORTANCIA DE FEATURES (SHAP)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Crear explainer\n",
    "    # Usar una muestra del dataset para acelerar el c√°lculo\n",
    "    sample_size = min(100, len(X_test))\n",
    "    X_sample = X_test.iloc[:sample_size]\n",
    "    \n",
    "    # Para modelos de sklearn (baselines)\n",
    "    explainer = shap.TreeExplainer(baselines['Random Forest'])\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    if task_type == 'classification' and len(shap_values) > 1:\n",
    "        shap.summary_plot(shap_values[1], X_sample, plot_type=\"bar\", show=False)\n",
    "    else:\n",
    "        shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "    plt.title('Importancia de Features (SHAP)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è SHAP no est√° instalado. Ejecute: !pip install shap\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error en an√°lisis SHAP: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Interpretaci√≥n de Negocios\n",
    "\n",
    "**Instrucciones:** Traduzca los resultados t√©cnicos a insights de negocio:\n",
    "\n",
    "---\n",
    "\n",
    "**Insights Principales:**\n",
    "1. *[Insight 1 - ¬øQu√© significa el resultado para el negocio?]*\n",
    "2. *[Insight 2]*\n",
    "3. *[Insight 3]*\n",
    "\n",
    "**Factores M√°s Importantes:**\n",
    "*[¬øCu√°les son los factores m√°s importantes seg√∫n el modelo y qu√© significan para el negocio?]*\n",
    "\n",
    "**Patrones Identificados:**\n",
    "*[¬øQu√© patrones ha identificado el modelo que pueden ser relevantes para la toma de decisiones?]*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Conclusiones y Recomendaciones de Negocio\n",
    "\n",
    "### 10.1 Resumen de Resultados\n",
    "\n",
    "**Instrucciones:** Proporcione un resumen ejecutivo de los resultados:\n",
    "\n",
    "---\n",
    "\n",
    "*[Resuma los principales resultados del proyecto en 2-3 p√°rrafos]*\n",
    "\n",
    "---\n",
    "\n",
    "### 10.2 Conclusiones\n",
    "\n",
    "**Instrucciones:** Liste las conclusiones principales:\n",
    "\n",
    "---\n",
    "\n",
    "1. *[Conclusi√≥n 1]*\n",
    "2. *[Conclusi√≥n 2]*\n",
    "3. *[Conclusi√≥n 3]*\n",
    "4. *[Conclusi√≥n 4]*\n",
    "\n",
    "---\n",
    "\n",
    "### 10.3 Recomendaciones de Negocio\n",
    "\n",
    "**Instrucciones:** Proporcione recomendaciones accionables basadas en los resultados:\n",
    "\n",
    "---\n",
    "\n",
    "**Recomendaciones a Corto Plazo:**\n",
    "1. *[Recomendaci√≥n 1]*\n",
    "2. *[Recomendaci√≥n 2]*\n",
    "\n",
    "**Recomendaciones a Mediano Plazo:**\n",
    "1. *[Recomendaci√≥n 1]*\n",
    "2. *[Recomendaci√≥n 2]*\n",
    "\n",
    "**Recomendaciones a Largo Plazo:**\n",
    "1. *[Recomendaci√≥n 1]*\n",
    "2. *[Recomendaci√≥n 2]*\n",
    "\n",
    "---\n",
    "\n",
    "### 10.4 Limitaciones del Estudio\n",
    "\n",
    "**Instrucciones:** Identifique las limitaciones de su an√°lisis:\n",
    "\n",
    "---\n",
    "\n",
    "1. *[Limitaci√≥n 1]*\n",
    "2. *[Limitaci√≥n 2]*\n",
    "3. *[Limitaci√≥n 3]*\n",
    "\n",
    "---\n",
    "\n",
    "### 10.5 Trabajo Futuro\n",
    "\n",
    "**Instrucciones:** Proponga l√≠neas de investigaci√≥n futura:\n",
    "\n",
    "---\n",
    "\n",
    "1. *[Propuesta 1]*\n",
    "2. *[Propuesta 2]*\n",
    "3. *[Propuesta 3]*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Referencias\n",
    "\n",
    "**Instrucciones:** Liste todas las referencias utilizadas (formato APA):\n",
    "\n",
    "---\n",
    "\n",
    "1. *[Referencia 1]*\n",
    "2. *[Referencia 2]*\n",
    "3. *[Referencia 3]*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Anexos\n",
    "\n",
    "### A. Guardado del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# GUARDAR EL MODELO ENTRENADO\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GUARDADO DEL MODELO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Guardar modelo PyTorch\n",
    "MODEL_PATH = 'modelo_final.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model_pytorch.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'history': history,\n",
    "    'hyperparameters': {\n",
    "        'input_size': INPUT_SIZE,\n",
    "        'hidden_sizes': HIDDEN_SIZES,\n",
    "        'output_size': OUTPUT_SIZE,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'learning_rate': LEARNING_RATE\n",
    "    }\n",
    "}, MODEL_PATH)\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo PyTorch guardado en: {MODEL_PATH}\")\n",
    "\n",
    "# Guardar modelo Keras (opcional)\n",
    "# model_keras.save('modelo_final.keras')\n",
    "# print(f\"‚úÖ Modelo Keras guardado en: modelo_final.keras\")\n",
    "\n",
    "# Guardar scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(f\"‚úÖ Scaler guardado en: scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Cargar Modelo Guardado (para Inferencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# CARGAR MODELO PARA INFERENCIA\n",
    "# =====================================================\n",
    "\n",
    "def load_model_and_predict(model_path, scaler_path, new_data):\n",
    "    \"\"\"\n",
    "    Carga el modelo entrenado y hace predicciones sobre nuevos datos.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Ruta al archivo del modelo\n",
    "        scaler_path: Ruta al archivo del scaler\n",
    "        new_data: DataFrame con los nuevos datos\n",
    "    \n",
    "    Returns:\n",
    "        Predicciones\n",
    "    \"\"\"\n",
    "    # Cargar checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Reconstruir modelo\n",
    "    hp = checkpoint['hyperparameters']\n",
    "    model = NeuralNetwork(\n",
    "        hp['input_size'], \n",
    "        hp['hidden_sizes'], \n",
    "        hp['output_size'], \n",
    "        hp['dropout_rate']\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Cargar scaler\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    \n",
    "    # Preprocesar datos\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "    new_data_tensor = torch.FloatTensor(new_data_scaled).to(device)\n",
    "    \n",
    "    # Hacer predicci√≥n\n",
    "    with torch.no_grad():\n",
    "        outputs = model(new_data_tensor)\n",
    "        if task_type == 'classification':\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            predictions = predictions.cpu().numpy()\n",
    "        else:\n",
    "            predictions = outputs.cpu().numpy().flatten()\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# predictions = load_model_and_predict('modelo_final.pth', 'scaler.pkl', new_df)\n",
    "print(\"‚úÖ Funci√≥n de carga e inferencia definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checklist de Entrega\n",
    "\n",
    "Antes de entregar, verifique que ha completado los siguientes elementos:\n",
    "\n",
    "- [ ] Informaci√≥n del proyecto completada\n",
    "- [ ] Resumen ejecutivo escrito\n",
    "- [ ] Problema de negocio claramente definido\n",
    "- [ ] Objetivos SMART establecidos\n",
    "- [ ] EDA completo con visualizaciones\n",
    "- [ ] Preprocesamiento de datos documentado\n",
    "- [ ] Arquitectura del modelo justificada\n",
    "- [ ] Modelo entrenado con curvas de aprendizaje\n",
    "- [ ] M√©tricas de evaluaci√≥n calculadas\n",
    "- [ ] Comparaci√≥n con modelos baseline\n",
    "- [ ] Interpretaci√≥n de resultados\n",
    "- [ ] Conclusiones y recomendaciones de negocio\n",
    "- [ ] Referencias listadas\n",
    "- [ ] C√≥digo ejecutable sin errores\n",
    "- [ ] Comentarios y documentaci√≥n adecuados\n",
    "\n",
    "---\n",
    "\n",
    "**¬°Buena suerte con su proyecto!** üéì"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
